{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "from vizdoom import gymnasium_wrapper\n",
    "from gymnasium.wrappers import RecordVideo\n",
    "from copy import deepcopy\n",
    "from torch.distributions import Normal\n",
    "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transition = np.dtype([\n",
    "#                        ('state_s', np.float32, (4, 240, 320)),\n",
    "#                        ('state_gv', np.float32, (5,)),\n",
    "#                        ('action_b', np.int16),\n",
    "#                        ('action_c', np.float32, (3,)),\n",
    "#                        ('reward', np.float32),\n",
    "#                        ('done', np.bool),\n",
    "#                        ('state_s_next', np.float32, (4, 240, 320)),\n",
    "#                        ('state_gv_next', np.float32, (5,)),\n",
    "#                       ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ActorNet, self).__init__()\n",
    "        self.img_stack = 4\n",
    "        self.cnn_base = nn.Sequential(  # input shape (4, 240, 320)\n",
    "            nn.Conv2d(self.img_stack, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 119, 159)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 59, 79)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 29, 39)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 14, 19)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 12, 17)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.AdaptiveAvgPool2d(output_size=1) # (256, 10, 15)\n",
    "        )  # output shape (256, 1, 1)\n",
    "        self.fc_base = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.binary = nn.Sequential(nn.Linear(320, 18), nn.Softmax(dim=1))\n",
    "        self.conts = nn.Sequential(nn.Linear(320, 3), nn.Tanh())\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = self.cnn_base(x)\n",
    "        x = x.view(-1, 256)\n",
    "        y = self.fc_base(y)\n",
    "        z = torch.cat((x, y), dim=1)\n",
    "        binary = self.binary(z)\n",
    "        conts = self.conts(z)\n",
    "        log_probs = self.conts(z)\n",
    "\n",
    "        return conts, log_probs, binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.img_stack = 4\n",
    "        self.cnn_state = nn.Sequential(  # input shape (4, 240, 320)\n",
    "            nn.Conv2d(self.img_stack, 8, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2),  # (8, 119, 159)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2),  # (16, 59, 79)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2),  # (32, 29, 39)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1),  # (64, 14, 19)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1),  # (128, 12, 17)\n",
    "            nn.ReLU(),  # activation\n",
    "            nn.AdaptiveAvgPool2d(output_size=1) # (256, 10, 15)\n",
    "        )  # output shape (256, 1, 1)\n",
    "        self.fc_state = nn.Sequential(\n",
    "            nn.Linear(5, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_binary = nn.Sequential(\n",
    "            nn.Linear(1, 18),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.fc_continiue = nn.Sequential(\n",
    "            nn.Linear(3, 32),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.value = nn.Sequential(nn.Linear(370, 1))\n",
    "\n",
    "    def forward(self, x, y, a, b):\n",
    "        x = self.cnn_state(x)\n",
    "        x = x.view(-1, 256)\n",
    "        y = self.fc_state(y)\n",
    "        a = self.fc_binary(a)\n",
    "        b = self.fc_continiue(b)\n",
    "        v = torch.cat((x, y, a, b), dim=1)\n",
    "        v = self.value(v)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC(nn.Module):\n",
    "    \n",
    "    def __init__(self, gamma=0.99, alpha=1e-3, tau=1e-2, \n",
    "                 buffer_capacity=256, batch_size=64, pi_lr=3e-4, q_lr=3e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi_model = ActorNet()\n",
    "        self.q1_model = ValueNet() \n",
    "        self.q2_model = ValueNet()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = [None] * self.buffer_capacity\n",
    "        self.counter = 0\n",
    "        self.pi_optimizer = torch.optim.Adam(self.pi_model.parameters(), pi_lr)\n",
    "        self.q1_optimizer = torch.optim.Adam(self.q1_model.parameters(), q_lr)\n",
    "        self.q2_optimizer = torch.optim.Adam(self.q2_model.parameters(), q_lr)\n",
    "        self.q1_target_model = deepcopy(self.q1_model)\n",
    "        self.q2_target_model = deepcopy(self.q2_model)\n",
    "\n",
    "    def get_action(self, state_s, state_gv):\n",
    "        state_s = torch.FloatTensor(state_s)\n",
    "        state_gv = torch.FloatTensor(state_gv)\n",
    "        action_c, log_probs, action_b = self.predict_actions(state_s, state_gv)\n",
    "        return action_b, action_c\n",
    "\n",
    "    def fit(self, state_s, state_gv, action_b, action_c, reward, done, state_s_next, state_gv_next):\n",
    "        self.memory[self.counter] = [state_s, state_gv, action_b, action_c, reward, done, state_s_next, state_gv_next]\n",
    "        self.counter += 1\n",
    "        if self.counter == self.buffer_capacity:\n",
    "            self.counter = 0\n",
    "        \n",
    "        if self.counter > self.batch_size:\n",
    "            batch = random.sample(self.memory[:self.counter], self.batch_size)\n",
    "            states_s, states_gv, actions_b, actions_c, rewards, dones, states_s_next, states_gv_next = zip(*batch)\n",
    "            \n",
    "            states_s = torch.tensor(np.stack(states_s)).float()\n",
    "            states_gv = torch.tensor(np.stack(states_gv)).float().squeeze(1)\n",
    "            actions_b = torch.tensor(np.stack(actions_b)).float()\n",
    "            actions_c = torch.tensor(np.stack(actions_c)).float()\n",
    "            rewards = torch.tensor(np.stack(rewards)).float()\n",
    "            dones = torch.tensor(np.stack(dones)).float()\n",
    "            states_s_next = torch.tensor(np.stack(states_s_next)).float()\n",
    "            states_gv_next = torch.tensor(np.stack(states_gv_next)).float().squeeze(1)\n",
    "            rewards, dones = rewards.unsqueeze(1), dones.unsqueeze(1)\n",
    "            \n",
    "            next_continious, next_log_probs, next_binarys = self.predict_actions(states_s_next, states_gv_next)\n",
    "            next_q1_values = self.q1_target_model(states_s_next, states_gv_next, next_binarys.float().unsqueeze(1), next_continious.float().squeeze(1))\n",
    "            next_q2_values = self.q2_target_model(states_s_next, states_gv_next, next_binarys.float().unsqueeze(1), next_continious.float().squeeze(1))\n",
    "            next_min_q_values = torch.min(next_q1_values, next_q2_values)\n",
    "            targets = rewards.detach().clone() + self.gamma * (1 - dones.detach().clone()) * (next_min_q_values.repeat(1, 3) - self.alpha * next_log_probs.squeeze(1))\n",
    "\n",
    "            q1_loss = torch.mean((self.q1_model(states_s, states_gv, actions_b.unsqueeze(1), actions_c) - targets) ** 2)\n",
    "            q2_loss = torch.mean((self.q2_model(states_s, states_gv, actions_b.unsqueeze(1), actions_c) - targets) ** 2)\n",
    "            self.update_model(q1_loss, self.q1_optimizer, self.q1_model, self.q1_target_model)\n",
    "            self.update_model(q2_loss, self.q2_optimizer, self.q2_model, self.q2_target_model)\n",
    "\n",
    "            pred_continious, log_probs, pred_binarys = self.predict_actions(states_s, states_gv)\n",
    "            q1_values = self.q1_model(states_s, states_gv, pred_binarys.float().unsqueeze(1), pred_continious.float().squeeze(1))\n",
    "            q2_values = self.q2_model(states_s, states_gv, pred_binarys.float().unsqueeze(1), pred_continious.float().squeeze(1))\n",
    "            \n",
    "            min_q_values = torch.min(q1_values, q2_values)\n",
    "            pi_loss = - torch.mean(min_q_values - self.alpha * log_probs)\n",
    "            self.update_model(pi_loss, self.pi_optimizer)\n",
    "            \n",
    "    def update_model(self, loss, optimizer, model=None, target_model=None):\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if model != None and target_model != None:\n",
    "            for param, target_param in zip(model.parameters(), target_model.parameters()):\n",
    "                # new_target_param = (1 - self.tau) * target_param + self.tau * param\n",
    "                target_param.data = (1 - self.tau) * target_param + self.tau * param\n",
    "\n",
    "    def predict_actions(self, states_s, states_gv):\n",
    "        means, log_stds, binary = self.pi_model(states_s, states_gv)\n",
    "        means, log_stds = means.unsqueeze(1), log_stds.unsqueeze(1)\n",
    "        dists = Normal(means, torch.exp(log_stds))\n",
    "        continious = dists.rsample()\n",
    "        log_probs = dists.log_prob(continious)\n",
    "        binary = torch.argmax(binary, dim=1)\n",
    "        return continious, log_probs, binary\n",
    "    \n",
    "    def save_param(self):\n",
    "        torch.save(self.pi_model.state_dict(), 'param/pi_model_params.pkl')\n",
    "        torch.save(self.q1_model.state_dict(), 'param/q1_model_params.pkl')\n",
    "        torch.save(self.q2_model.state_dict(), 'param/q2_model_params.pkl')\n",
    "\n",
    "    def load_param(self):\n",
    "        self.pi_model.load_state_dict(torch.load('param/pi_model_params.pkl'))\n",
    "        self.q1_model.load_state_dict(torch.load('param/q1_model_params.pkl'))\n",
    "        self.q2_model.load_state_dict(torch.load('param/q2_model_params.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Available buttons                           # Доступные кнопки\n",
    "# available_buttons =                         # доступные_кнопки =\n",
    "# {                                           # {\n",
    "#     ATTACK                                  #     АТАКА\n",
    "#     SPEED                                   #     СКОРОСТЬ\n",
    "#     STRAFE                                  #     ОБХОД\n",
    "\n",
    "#     MOVE_RIGHT                              #     ДВИЖЕНИЕ_ВПРАВО\n",
    "#     MOVE_LEFT                               #     ДВИЖЕНИЕ_ВЛЕВО\n",
    "#     MOVE_BACKWARD                           #     ДВИЖЕНИЕ_НАЗАД\n",
    "#     MOVE_FORWARD                            #     ДВИЖЕНИЕ_ВПЕРЕД\n",
    "#     TURN_RIGHT                              #     ПОВЕРНИ_ВПРАВО\n",
    "#     TURN_LEFT                               #     ПОВЕРНИ_ВЛЕВО\n",
    "\n",
    "#     SELECT_WEAPON1                          #     ВЫБЕРИ_ОРУЖИЕ 1\n",
    "#     SELECT_WEAPON2                          #     ВЫБЕРИ_ОРУЖИЕ 2\n",
    "#     SELECT_WEAPON3                          #     ВЫБЕРИ_ОРУЖИЕ 3\n",
    "#     SELECT_WEAPON4                          #     ВЫБЕРИ_ОРУЖИЕ 4\n",
    "#     SELECT_WEAPON5                          #     ВЫБЕРИ_ОРУЖИЕ 5\n",
    "#     SELECT_WEAPON6                          #     ВЫБЕРИ_ОРУЖИЕ 6\n",
    "\n",
    "#     SELECT_NEXT_WEAPON                      #     ВЫБОР_СЛЕДУЮЩЕГО_ОРУЖИЯ\n",
    "#     SELECT_PREV_WEAPON                      #     ВЫБОР_ПРЕДЫДУЩЕГО_ОРУЖИЯ\n",
    "\n",
    "#     LOOK_UP_DOWN_DELTA                      #     СМОТРЕТЬ_ВВЕРХ_ВНИЗ_ПО_ДЕЛЬТЕ\n",
    "#     TURN_LEFT_RIGHT_DELTA                   #     ПОВЕРНУТЬ_НАЛЕВО_НАПРАВО_ТРЕУГОЛЬНИКОМ\n",
    "#     MOVE_LEFT_RIGHT_DELTA                   #     ПЕРЕМЕЩЕНИЕ_ВЛЕВО_ВПРАВО_ДЕЛЬТА\n",
    "\n",
    "# }                                           # }\n",
    "\n",
    "# Game variables that will be in the state    # Игровые переменные, которые будут находиться в состоянии\n",
    "# available_game_variables =                  # доступные игровые переменные =\n",
    "# {                                           # {\n",
    "#     KILLCOUNT                               #     КОЛИЧЕСТВО УБИТЫХ\n",
    "#     HEALTH                                  #     ЗДОРОВЬЕ\n",
    "#     ARMOR                                   #     БРОНЯ\n",
    "#     SELECTED_WEAPON                         #     ВЫБРАННОЕ_ОРУЖИЕ\n",
    "#     SELECTED_WEAPON_AMMO                    #     ВЫБРАННОЕ_ОРУЖИЕ_АММО\n",
    "# }                                           # }\n",
    "# mode = PLAYER                               # режим = ИГРОК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = gym.make('VizdoomDeathmatch-v0')\n",
    "        self.seed = 123\n",
    "        self.img_stack = 4\n",
    "        self.game_var = 1\n",
    "        self.action_repeat = 8\n",
    "        self.gamevariables_prev = [0., 100., 0., 2., 50.]\n",
    "\n",
    "    def reset(self):\n",
    "        self.counter = 0\n",
    "        self.done = False\n",
    "        state, _ = self.env.reset()\n",
    "\n",
    "        img_rgb = state['screen']\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack = [img_gray] * self.img_stack  # four frames for decision\n",
    "\n",
    "        gamevariables = state['gamevariables']\n",
    "        self.gv_stack = [gamevariables] * self.game_var  # four frames for decision\n",
    "        state_s = np.array(self.stack)\n",
    "        state_gv = np.array(self.gv_stack)\n",
    "        # print('Start', state['gamevariables']) # [  0. 100.   0.   2.  50.]\n",
    "        return state_s, state_gv\n",
    "        \n",
    "    #  KILLCOUNT HEALTH   ARMOR SELECTED_WEAPON SELECTED_WEAPON_AMMO\n",
    "    # [killcount health   armor weapon ammo      ]\n",
    "    # [кол-во    здоровье броня оружие боеприпасы]\n",
    "    # [0.        100.     0.    2.     50.       ]\n",
    "\n",
    "    def step(self, action_b, action_c):\n",
    "        action = {'binary': action_b, 'continuous': action_c}\n",
    "        state, reward, done, _, _ = self.env.step(action)\n",
    "        img_rgb = state['screen']\n",
    "        gamevariables = state['gamevariables']\n",
    "        self.gamevariables_prev = gamevariables\n",
    "        img_gray = self.rgb2gray(img_rgb)\n",
    "        self.stack.pop(0)\n",
    "        self.stack.append(img_gray)\n",
    "        self.gv_stack.pop(0)\n",
    "        self.gv_stack.append(gamevariables)\n",
    "        assert len(self.stack) == self.img_stack\n",
    "        assert len(self.gv_stack) == self.game_var\n",
    "        return np.array(self.stack), np.array(self.gv_stack), reward, done\n",
    "\n",
    "    def render(self, *arg):\n",
    "        self.env.render(*arg)\n",
    "\n",
    "    @staticmethod\n",
    "    def rgb2gray(rgb, norm=True):\n",
    "        gray = np.dot(rgb[..., :], [0.299, 0.587, 0.114]) # rgb image -> gray [0, 1]\n",
    "        if norm:\n",
    "            gray = gray / 128. - 1. # normalize\n",
    "        return gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = SAC()\n",
    "\n",
    "episode_n = 100\n",
    "reward_max = 0\n",
    "total_rewards = []\n",
    "reward_threshold = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in range(episode_n):\n",
    "\n",
    "    total_reward = 0\n",
    "    state_s, state_gv = env.reset()\n",
    "    \n",
    "    for t in range(2000):\n",
    "        # _, _ = env.reset()\n",
    "        action_b, action_c = agent.get_action(state_s, state_gv)\n",
    "        action_b = action_b.squeeze().detach().numpy()\n",
    "        action_c = action_c.squeeze().detach().numpy()\n",
    "        state_s_next, state_gv_next, reward, done = env.step(action_b, action_c)\n",
    "\n",
    "        agent.fit(state_s, state_gv, action_b, action_c, reward, done, state_s_next, state_gv_next)\n",
    "        \n",
    "        total_reward += reward\n",
    "        state_s = state_s_next\n",
    "        state_gv = state_gv_next\n",
    "\n",
    "        # print('Шаг', t+1, end='\\r')\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "    if episode < 50:\n",
    "        reward_50 = np.mean(total_rewards[:episode])\n",
    "    else:\n",
    "        reward_50 = np.mean(total_rewards[episode-50:episode])\n",
    "    if reward_max < reward_50:\n",
    "        reward_max = reward_50\n",
    "        agent.save_param()\n",
    "        print(f'Эпизод {episode+1}\\tСохранение агента,\\treward-50-max: {reward_max:.2f}                                         ')\n",
    "    \n",
    "    print(f'Эпизод {episode+1}\\tПоследний reward: {total_reward:.2f}\\treward-50: {reward_50:.2f}\\tt: {t+1}              ', end='\\r')\n",
    "\n",
    "    if reward_50 > reward_threshold:\n",
    "        print(f'Эпизод {episode+1}\\tОбучение остановлено. \\tПоследний score: {total_reward:.2f}\\treward-50-max: {reward_max:.2f}')\n",
    "        break\n",
    "\n",
    "print('\\n', reward_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_param()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recent 50 interval average\n",
    "average_reward = []\n",
    "for idx in range(20, len(total_rewards)):\n",
    "    avg_list = np.empty(shape=(1,), dtype=int)\n",
    "    if idx < 50:\n",
    "        avg_list = total_rewards[:idx+1]\n",
    "    else:\n",
    "        avg_list = total_rewards[idx-49:idx+1]\n",
    "    average_reward.append(np.average(avg_list))\n",
    "plt.plot(total_rewards)\n",
    "plt.plot(average_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование агента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PIL\n",
    "import shutil as sh\n",
    "from glob import glob\n",
    "from base64 import b64encode\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dir = './vizdoom-videos'\n",
    "scenarios_dir = env_dir # os.path.join(env_dir, 'scenarios')\n",
    "if not os.path.exists(scenarios_dir):\n",
    "    os.makedirs(scenarios_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_episode(eps_frames, records_dir, exp_id, eps_num):\n",
    "    record_path = os.path.join(records_dir, f'record-{exp_id}_eps-{eps_num}.mp4')\n",
    "    eps_frame_dir = './vizdoom-videos/episode_frames'\n",
    "    if not os.path.exists(eps_frame_dir):\n",
    "        os.mkdir(eps_frame_dir)\n",
    "\n",
    "    for i, frame  in enumerate(eps_frames):\n",
    "        PIL.Image.fromarray(frame).save(os.path.join(eps_frame_dir, f'frame-{i+1}.png'))\n",
    "\n",
    "    os.system(f'ffmpeg -r 30 -i {eps_frame_dir}/frame-%1d.png -vcodec libx264 -b 10M -y \"{record_path}\"')\n",
    "    # sh.rmtree(eps_frame_dir)\n",
    "\n",
    "\n",
    "def show_episode_records(records_dir):\n",
    "    record_paths = glob(os.path.join(records_dir, \"*.mp4\"))\n",
    "    html_str = ''\n",
    "    for i, record_path in enumerate(record_paths):\n",
    "        mp4 = open(record_path, 'rb').read()\n",
    "        data = f\"data:video/mp4;base64,{b64encode(mp4).decode()}\"\n",
    "        html_str += f'EPISODE # {i+1}<br><video width=500 controls><source src=\"{data}\" type=\"video/mp4\"></video><br><br>'\n",
    "    return HTML(html_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, agent):\n",
    "    state, img_rgb = env.reset()\n",
    "    step_count = 0\n",
    "    frames = list()\n",
    "    done = False\n",
    "    frames.append(img_rgb)\n",
    "    while not done:\n",
    "        # print(type(state['screen']))\n",
    "        action, _ = agent.select_action(state)\n",
    "        next_screen, next_gamevariables, reward, done, img_rgb_, = env.step(action) # terminated, truncated,\n",
    "        frames.append(img_rgb_)\n",
    "        # done = terminated or truncated\n",
    "        state = {'screen': next_screen, 'gamevariables': next_gamevariables}\n",
    "        step_count += 1\n",
    "\n",
    "    # frames.extend([env.render()] * 3)\n",
    "    return frames\n",
    "\n",
    "def evaluate_agent(env, agent, exp_dir, n_eval=1):\n",
    "    for eps_num in range(1, n_eval + 1):\n",
    "        eps_frames = run_episode(env, agent)\n",
    "        print('frames =', len(eps_frames))\n",
    "        # record_episode(eps_frames, exp_dir, f'exp-{datetime.now().strftime(\"%d%m-%H%M%S\")}', eps_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = Env() # gym.make('VizdoomDeathmatch-v0', render_mode='rgb_array')\n",
    "evaluate_agent(env, agent, env_dir)\n",
    "# env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import ImageSequenceClip\n",
    "eps_frames = run_episode(env, agent)\n",
    "record_path = os.path.join(env_dir, f'record-exp-{datetime.now().strftime(\"%d%m-%H%M%S\")}.mp4')\n",
    "# Настройки\n",
    "fps = 24          # Частота кадров в секунду\n",
    "\n",
    "# Сборка клипа из изображений\n",
    "clip = ImageSequenceClip(eps_frames, fps=fps)\n",
    "\n",
    "# Сохранение видеофайла\n",
    "clip.write_videofile(record_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_episode_records(env_dir) # exp_dirs['evaluation']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка окружения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.6724, 1.5521, 1.6525]], grad_fn=<AddBackward0>),\n",
       " tensor([[1.7859, 1.7419, 1.6252]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example = torch.rand(4,240,320)\n",
    "net_a = ActorNet()\n",
    "net_a(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0629]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_example = torch.rand(4,240,320)\n",
    "net_v = ValueNet()\n",
    "net_v(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_doom = gym.make('VizdoomDeathmatch-v0', render_mode=\"rgb_array\")\n",
    "action_doom = env_doom.action_space\n",
    "obs_doom = env_doom.observation_space\n",
    "print('action', action_doom)\n",
    "print('obs', obs_doom)\n",
    "print(env_doom.spec.reward_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_doom = env_doom.reset()\n",
    "random_action = action_doom.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ods-rl-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
